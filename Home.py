"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                              â•‘
â•‘        â¤ï¸  UCI HEART DISEASE PREDICTION - COMPREHENSIVE STREAMLIT APP       â•‘
â•‘                                                                              â•‘
â•‘                    TÃ¼m 6 Senaryo Ã— Tamamen Entegre Uygulama                â•‘
â•‘                                                                              â•‘
â•‘  Ã–zellikler:                                                                â•‘
â•‘  âœ… S0-S5: 6 senaryo tam analiz                                             â•‘
â•‘  âœ… Senaryo karÅŸÄ±laÅŸtÄ±rmasÄ± ve heatmap                                       â•‘
â•‘  âœ… Hasta prediksiyon modÃ¼lÃ¼                                                 â•‘
â•‘  âœ… Model seÃ§imi Ã¶nerileri                                                   â•‘
â•‘  âœ… Teknik dokumentasyon ve aÃ§Ä±klamalar                                      â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Preprocessing
from sklearn.impute import KNNImputer
from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler
from sklearn.decomposition import PCA

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier

# Validation & Metrics
from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict
from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score, f1_score, recall_score, accuracy_score

# Class Imbalance
from imblearn.over_sampling import SMOTE

# Optimization
import optuna
optuna.logging.set_verbosity(optuna.logging.WARNING)

# ============================================================================
# PAGE CONFIGURATION & STYLING
# ============================================================================

st.set_page_config(
    page_title="â¤ï¸ Heart Disease - Comprehensive Analysis",
    page_icon="â¤ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .main { padding-top: 0px; }
    .metric-box {
        background-color: #f0f2f6;
        padding: 15px;
        border-radius: 10px;
        margin: 10px 0;
    }
    .scenario-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 20px;
        border-radius: 10px;
        margin: 20px 0;
    }
    .scenario-header-alt {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        color: white;
        padding: 20px;
        border-radius: 10px;
        margin: 20px 0;
    }
    .model-result-good {
        background-color: #d4edda;
        padding: 10px;
        border-left: 4px solid #28a745;
        border-radius: 5px;
    }
    .model-result-bad {
        background-color: #f8d7da;
        padding: 10px;
        border-left: 4px solid #dc3545;
        border-radius: 5px;
    }
    .info-box {
        background-color: #d1ecf1;
        border-left: 4px solid #0c5460;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
    .success-box {
        background-color: #d4edda;
        border-left: 4px solid #155724;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
</style>
""", unsafe_allow_html=True)

# ============================================================================
# DATA LOADING AND PREPROCESSING FUNCTIONS
# ============================================================================

@st.cache_data
def load_cleveland_data():
    """Load Cleveland dataset"""
    try:
        df = pd.read_csv("data/heart_disease_uci.csv")
        df = df[df['dataset'] == 'Cleveland'].copy()
        df['target'] = (df['num'] > 0).astype(int)
        return df
    except:
        st.error("âŒ Dataset yÃ¼klenemedi!")
        return None

@st.cache_data
def basic_preprocessing(df):
    """Common preprocessing for all scenarios"""
    df_processed = df.copy()
    
    categorical_cols = ['sex', 'cp', 'restecg', 'exang', 'slope', 'thal', 'fbs']
    
    for col in categorical_cols:
        if col in df_processed.columns:
            le = LabelEncoder()
            df_processed[col] = df_processed[col].fillna('missing')
            df_processed[col] = le.fit_transform(df_processed[col].astype(str))
    
    exclude_cols = ['id', 'num', 'target', 'dataset']
    numeric_cols = [col for col in df_processed.select_dtypes(include=[np.number]).columns 
                   if col not in exclude_cols]
    
    imputer = KNNImputer(n_neighbors=5)
    df_processed[numeric_cols] = imputer.fit_transform(df_processed[numeric_cols])
    
    return df_processed

def add_feature_engineering(df):
    """Add engineered features"""
    df_fe = df.copy()
    
    df_fe['risk_score'] = (df_fe['age'] * df_fe['chol']) / 10000
    df_fe['age_group'] = pd.cut(
        df_fe['age'], 
        bins=[0, 40, 55, 70, 100],
        labels=[0, 1, 2, 3]
    ).astype(float).fillna(1).astype(int)
    df_fe['hr_age_ratio'] = df_fe['thalch'] / (df_fe['age'] + 1)
    df_fe['bp_chol_interaction'] = (df_fe['trestbps'] * df_fe['chol']) / 10000
    
    return df_fe

def get_features_target(df, exclude_extra=[]):
    """Extract features and target"""
    exclude_cols = ['id', 'num', 'target', 'dataset'] + exclude_extra
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    X = df[feature_cols].values
    y = df['target'].values
    
    return X, y, feature_cols

def get_default_models():
    """Get all models"""
    return {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),
        'SVM': SVC(probability=True, random_state=42),
        'Naive Bayes': GaussianNB(),
        'XGBoost': XGBClassifier(random_state=42, n_jobs=-1, use_label_encoder=False, 
                                eval_metric='logloss', verbosity=0),
        'KNN': KNeighborsClassifier(n_jobs=-1)
    }

# ============================================================================
# MODEL EVALUATION
# ============================================================================

def evaluate_all_models(X, y, models=None, cv=10):
    """Evaluate all models with 10-Fold CV"""
    if models is None:
        models = get_default_models()
    
    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)
    results = {}
    
    progress_bar = st.progress(0)
    total_models = len(models)
    
    for idx, (name, model) in enumerate(models.items()):
        try:
            acc = cross_val_score(model, X, y, cv=skf, scoring='accuracy')
            f1 = cross_val_score(model, X, y, cv=skf, scoring='f1')
            rec = cross_val_score(model, X, y, cv=skf, scoring='recall')
            auc_score = cross_val_score(model, X, y, cv=skf, scoring='roc_auc')
            
            results[name] = {
                'accuracy': f"{acc.mean():.3f}Â±{acc.std():.3f}",
                'f1': f"{f1.mean():.3f}Â±{f1.std():.3f}",
                'recall': f"{rec.mean():.3f}Â±{rec.std():.3f}",
                'auc': f"{auc_score.mean():.3f}Â±{auc_score.std():.3f}",
                'f1_mean': f1.mean(),
                'acc_mean': acc.mean(),
                'recall_mean': rec.mean(),
                'auc_mean': auc_score.mean()
            }
        except Exception as e:
            results[name] = {
                'accuracy': "N/A", 'f1': "N/A", 'recall': "N/A", 'auc': "N/A",
                'f1_mean': 0, 'acc_mean': 0, 'recall_mean': 0, 'auc_mean': 0
            }
        
        progress_bar.progress((idx + 1) / total_models)
    
    return results

# ============================================================================
# SCENARIO IMPLEMENTATIONS
# ============================================================================

@st.cache_data
def scenario_0_baseline(df):
    """S0: Baseline"""
    df_processed = basic_preprocessing(df)
    X, y, features = get_features_target(df_processed)
    
    scaler = RobustScaler()
    X_scaled = scaler.fit_transform(X)
    
    models = get_default_models()
    results = evaluate_all_models(X_scaled, y, models)
    
    return results, features, "RobustScaler"

@st.cache_data
def scenario_1_pca(df):
    """S1: + PCA"""
    df_processed = basic_preprocessing(df)
    X, y, features = get_features_target(df_processed)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    pca = PCA(n_components=0.95, random_state=42)
    X_pca = pca.fit_transform(X_scaled)
    
    models = get_default_models()
    results = evaluate_all_models(X_pca, y, models)
    
    pca_info = f"13 features â†’ {X_pca.shape[1]} components ({pca.explained_variance_ratio_.sum():.1%} variance)"
    
    return results, pca_info

@st.cache_data
def scenario_2_feature_engineering(df):
    """S2: + Feature Engineering"""
    df_processed = basic_preprocessing(df)
    df_fe = add_feature_engineering(df_processed)
    X, y, features = get_features_target(df_fe)
    
    scaler = RobustScaler()
    X_scaled = scaler.fit_transform(X)
    
    models = get_default_models()
    results = evaluate_all_models(X_scaled, y, models)
    
    return results, features

@st.cache_data
def scenario_3_smote(df):
    """S3: + SMOTE"""
    df_processed = basic_preprocessing(df)
    X, y, features = get_features_target(df_processed)
    
    scaler = RobustScaler()
    X_scaled = scaler.fit_transform(X)
    
    smote = SMOTE(random_state=42)
    X_smote, y_smote = smote.fit_resample(X_scaled, y)
    
    models = get_default_models()
    results = evaluate_all_models(X_smote, y_smote, models)
    
    balance_info = f"{sum(y==0)} vs {sum(y==1)} â†’ {sum(y_smote==0)} vs {sum(y_smote==1)}"
    
    return results, balance_info

@st.cache_data
def scenario_4_optuna(df):
    """S4: + Optuna"""
    df_processed = basic_preprocessing(df)
    X, y, features = get_features_target(df_processed)
    
    scaler = RobustScaler()
    X_scaled = scaler.fit_transform(X)
    
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    
    results = {}
    best_params_dict = {}
    
    st.info("â³ Optuna optimizasyonu Ã§alÄ±ÅŸÄ±yor (20 trial per model)...")
    
    # LR
    try:
        def objective_lr(trial):
            C = trial.suggest_float('C', 0.01, 10.0, log=True)
            penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])
            model = LogisticRegression(C=C, penalty=penalty, solver='lbfgs', 
                                     max_iter=1000, random_state=42)
            return cross_val_score(model, X_scaled, y, cv=skf, scoring='f1').mean()
        
        study = optuna.create_study(direction='maximize')
        study.optimize(objective_lr, n_trials=15, show_progress_bar=False)
        best_model = LogisticRegression(**study.best_params, solver='lbfgs', 
                                       max_iter=1000, random_state=42)
        best_params_dict['Logistic Regression'] = study.best_params
        
        acc = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='accuracy')
        f1 = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='f1')
        rec = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='recall')
        auc_score = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='roc_auc')
        
        results['Logistic Regression'] = {
            'accuracy': f"{acc.mean():.3f}Â±{acc.std():.3f}",
            'f1': f"{f1.mean():.3f}Â±{f1.std():.3f}",
            'recall': f"{rec.mean():.3f}Â±{rec.std():.3f}",
            'auc': f"{auc_score.mean():.3f}Â±{auc_score.std():.3f}",
            'f1_mean': f1.mean(), 'acc_mean': acc.mean(),
            'recall_mean': rec.mean(), 'auc_mean': auc_score.mean()
        }
    except:
        results['Logistic Regression'] = {
            'accuracy': "N/A", 'f1': "N/A", 'recall': "N/A", 'auc': "N/A",
            'f1_mean': 0, 'acc_mean': 0, 'recall_mean': 0, 'auc_mean': 0
        }
    
    # RF
    try:
        def objective_rf(trial):
            n_estimators = trial.suggest_int('n_estimators', 50, 200)
            max_depth = trial.suggest_int('max_depth', 3, 15)
            min_samples_split = trial.suggest_int('min_samples_split', 2, 15)
            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                          min_samples_split=min_samples_split,
                                          random_state=42, n_jobs=-1)
            return cross_val_score(model, X_scaled, y, cv=skf, scoring='f1').mean()
        
        study = optuna.create_study(direction='maximize')
        study.optimize(objective_rf, n_trials=15, show_progress_bar=False)
        best_model = RandomForestClassifier(**study.best_params, random_state=42, n_jobs=-1)
        best_params_dict['Random Forest'] = study.best_params
        
        acc = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='accuracy')
        f1 = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='f1')
        rec = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='recall')
        auc_score = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='roc_auc')
        
        results['Random Forest'] = {
            'accuracy': f"{acc.mean():.3f}Â±{acc.std():.3f}",
            'f1': f"{f1.mean():.3f}Â±{f1.std():.3f}",
            'recall': f"{rec.mean():.3f}Â±{rec.std():.3f}",
            'auc': f"{auc_score.mean():.3f}Â±{auc_score.std():.3f}",
            'f1_mean': f1.mean(), 'acc_mean': acc.mean(),
            'recall_mean': rec.mean(), 'auc_mean': auc_score.mean()
        }
    except:
        results['Random Forest'] = {
            'accuracy': "N/A", 'f1': "N/A", 'recall': "N/A", 'auc': "N/A",
            'f1_mean': 0, 'acc_mean': 0, 'recall_mean': 0, 'auc_mean': 0
        }
    
    # SVM
    try:
        def objective_svm(trial):
            C = trial.suggest_float('C', 0.1, 100.0, log=True)
            kernel = trial.suggest_categorical('kernel', ['rbf', 'poly'])
            model = SVC(C=C, kernel=kernel, probability=True, random_state=42)
            return cross_val_score(model, X_scaled, y, cv=skf, scoring='f1').mean()
        
        study = optuna.create_study(direction='maximize')
        study.optimize(objective_svm, n_trials=15, show_progress_bar=False)
        best_model = SVC(**study.best_params, probability=True, random_state=42)
        best_params_dict['SVM'] = study.best_params
        
        acc = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='accuracy')
        f1 = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='f1')
        rec = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='recall')
        auc_score = cross_val_score(best_model, X_scaled, y, cv=skf, scoring='roc_auc')
        
        results['SVM'] = {
            'accuracy': f"{acc.mean():.3f}Â±{acc.std():.3f}",
            'f1': f"{f1.mean():.3f}Â±{f1.std():.3f}",
            'recall': f"{rec.mean():.3f}Â±{rec.std():.3f}",
            'auc': f"{auc_score.mean():.3f}Â±{auc_score.std():.3f}",
            'f1_mean': f1.mean(), 'acc_mean': acc.mean(),
            'recall_mean': rec.mean(), 'auc_mean': auc_score.mean()
        }
    except:
        results['SVM'] = {
            'accuracy': "N/A", 'f1': "N/A", 'recall': "N/A", 'auc': "N/A",
            'f1_mean': 0, 'acc_mean': 0, 'recall_mean': 0, 'auc_mean': 0
        }
    
    # NB, XGB, KNN dengan default params
    for model_name in ['Naive Bayes', 'XGBoost', 'KNN']:
        try:
            model = get_default_models()[model_name]
            acc = cross_val_score(model, X_scaled, y, cv=skf, scoring='accuracy')
            f1 = cross_val_score(model, X_scaled, y, cv=skf, scoring='f1')
            rec = cross_val_score(model, X_scaled, y, cv=skf, scoring='recall')
            auc_score = cross_val_score(model, X_scaled, y, cv=skf, scoring='roc_auc')
            
            results[model_name] = {
                'accuracy': f"{acc.mean():.3f}Â±{acc.std():.3f}",
                'f1': f"{f1.mean():.3f}Â±{f1.std():.3f}",
                'recall': f"{rec.mean():.3f}Â±{rec.std():.3f}",
                'auc': f"{auc_score.mean():.3f}Â±{auc_score.std():.3f}",
                'f1_mean': f1.mean(), 'acc_mean': acc.mean(),
                'recall_mean': rec.mean(), 'auc_mean': auc_score.mean()
            }
        except:
            results[model_name] = {
                'accuracy': "N/A", 'f1': "N/A", 'recall': "N/A", 'auc': "N/A",
                'f1_mean': 0, 'acc_mean': 0, 'recall_mean': 0, 'auc_mean': 0
            }
    
    return results, best_params_dict

@st.cache_data
def scenario_5_all_combined(df):
    """S5: All Combined"""
    df_processed = basic_preprocessing(df)
    df_fe = add_feature_engineering(df_processed)
    X, y, features = get_features_target(df_fe)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    pca = PCA(n_components=0.95, random_state=42)
    X_pca = pca.fit_transform(X_scaled)
    
    smote = SMOTE(random_state=42)
    X_combined, y_combined = smote.fit_resample(X_pca, y)
    
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    
    results = {}
    
    test_models = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
        'XGBoost': XGBClassifier(random_state=42, n_jobs=-1, use_label_encoder=False, 
                               eval_metric='logloss', verbosity=0)
    }
    
    st.info("â³ S5 optimizasyonu Ã§alÄ±ÅŸÄ±yor (20 trial per model)...")
    
    for model_name, model in test_models.items():
        try:
            if model_name == 'Logistic Regression':
                def objective(trial):
                    C = trial.suggest_float('C', 0.01, 10.0, log=True)
                    penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])
                    m = LogisticRegression(C=C, penalty=penalty, solver='lbfgs',
                                         max_iter=1000, random_state=42)
                    return cross_val_score(m, X_combined, y_combined, cv=skf, scoring='f1').mean()
                
                study = optuna.create_study(direction='maximize')
                study.optimize(objective, n_trials=15, show_progress_bar=False)
                model = LogisticRegression(**study.best_params, solver='lbfgs',
                                         max_iter=1000, random_state=42)
            
            elif model_name == 'XGBoost':
                def objective(trial):
                    n_estimators = trial.suggest_int('n_estimators', 50, 200)
                    max_depth = trial.suggest_int('max_depth', 2, 10)
                    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)
                    m = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                    learning_rate=learning_rate, random_state=42,
                                    n_jobs=-1, use_label_encoder=False, eval_metric='logloss')
                    return cross_val_score(m, X_combined, y_combined, cv=skf, scoring='f1').mean()
                
                study = optuna.create_study(direction='maximize')
                study.optimize(objective, n_trials=15, show_progress_bar=False)
                model = XGBClassifier(**study.best_params, random_state=42, n_jobs=-1,
                                    use_label_encoder=False, eval_metric='logloss')
            
            acc = cross_val_score(model, X_combined, y_combined, cv=skf, scoring='accuracy')
            f1 = cross_val_score(model, X_combined, y_combined, cv=skf, scoring='f1')
            rec = cross_val_score(model, X_combined, y_combined, cv=skf, scoring='recall')
            auc_score = cross_val_score(model, X_combined, y_combined, cv=skf, scoring='roc_auc')
            
            results[model_name] = {
                'accuracy': f"{acc.mean():.3f}Â±{acc.std():.3f}",
                'f1': f"{f1.mean():.3f}Â±{f1.std():.3f}",
                'recall': f"{rec.mean():.3f}Â±{rec.std():.3f}",
                'auc': f"{auc_score.mean():.3f}Â±{auc_score.std():.3f}",
                'f1_mean': f1.mean(),
                'acc_mean': acc.mean(),
                'recall_mean': rec.mean(),
                'auc_mean': auc_score.mean()
            }
        except Exception as e:
            results[model_name] = {
                'accuracy': "N/A", 'f1': "N/A", 'recall': "N/A", 'auc': "N/A",
                'f1_mean': 0, 'acc_mean': 0, 'recall_mean': 0, 'auc_mean': 0
            }
    
    pipeline_info = f"17 features â†’ FE â†’ StandardScaler â†’ PCA: {X_pca.shape[1]} â†’ SMOTE: {len(X_combined)}"
    
    return results, pipeline_info

# ============================================================================
# VISUALIZATION FUNCTIONS
# ============================================================================

def plot_results_table(results):
    """Display results table"""
    data = []
    for model_name, metrics in results.items():
        data.append({
            'Model': model_name,
            'Accuracy': metrics['accuracy'],
            'Recall': metrics['recall'],
            'F1-Score': metrics['f1'],
            'AUC': metrics['auc']
        })
    
    df_results = pd.DataFrame(data)
    st.dataframe(df_results, use_container_width=True)
    
    return df_results

def plot_f1_comparison(results, title="F1-Score Comparison"):
    """Plot F1 scores"""
    valid_results = {k: v for k, v in results.items() if v['f1_mean'] > 0}
    
    if not valid_results:
        st.warning("No valid results to plot")
        return
    
    models = list(valid_results.keys())
    f1_scores = [valid_results[m]['f1_mean'] for m in models]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    colors = ['#2ecc71' if f > 0.80 else '#f39c12' if f > 0.75 else '#e74c3c' for f in f1_scores]
    bars = ax.barh(models, f1_scores, color=colors, edgecolor='black', linewidth=1.5)
    ax.set_xlabel('F1-Score', fontsize=12, fontweight='bold')
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.set_xlim(0, 1)
    ax.grid(axis='x', alpha=0.3)
    
    for i, (bar, score) in enumerate(zip(bars, f1_scores)):
        ax.text(score + 0.01, i, f'{score:.3f}', va='center', fontsize=10, fontweight='bold')
    
    plt.tight_layout()
    st.pyplot(fig, use_container_width=True)

def plot_metrics_comparison(results, title="Metrics Comparison"):
    """Compare all metrics"""
    valid_results = {k: v for k, v in results.items() if v['f1_mean'] > 0}
    
    if not valid_results:
        st.warning("No valid results to plot")
        return
    
    models = list(valid_results.keys())
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle(title, fontsize=14, fontweight='bold')
    
    metrics = ['acc_mean', 'recall_mean', 'f1_mean', 'auc_mean']
    metric_names = ['Accuracy', 'Recall', 'F1-Score', 'AUC']
    
    for idx, (ax, metric, metric_name) in enumerate(zip(axes.flat, metrics, metric_names)):
        values = [valid_results[m][metric] for m in models]
        colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(models)))
        ax.barh(models, values, color=colors, edgecolor='black', linewidth=1.5)
        ax.set_xlabel(metric_name, fontsize=11, fontweight='bold')
        ax.set_xlim(0, 1)
        ax.grid(axis='x', alpha=0.3)
        
        for i, v in enumerate(values):
            ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=9, fontweight='bold')
    
    plt.tight_layout()
    st.pyplot(fig, use_container_width=True)

# ============================================================================
# PAGE BUILDERS
# ============================================================================

def page_home():
    """Home/Welcome page"""
    st.markdown("""
    <div class="scenario-header">
        <h1>â¤ï¸ UCI Heart Disease Prediction</h1>
        <h3>KapsamlÄ± Senaryo Analiz ve Hasta Tahmini Sistemi</h3>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    ### ğŸ“Š Proje TanÄ±mÄ±
    
    Bu uygulama, UCI Heart Disease veri seti (Cleveland) Ã¼zerinde **6 farklÄ± senaryo analizi** gerÃ§ekleÅŸtirmektedir.
    Her senaryo, farklÄ± veri Ã¶niÅŸleme ve optimizasyon tekniklerini kapsamaktadÄ±r.
    
    **Proje AmacÄ±:**
    1. FarklÄ± tekniklerin model performansÄ±na etkisini izole olarak gÃ¶rmek (Ablation Study)
    2. Hiperparametre optimizasyonu ile model performansÄ±nÄ± maksimize etmek
    3. 6 farklÄ± makine Ã¶ÄŸrenmesi algoritmasÄ±nÄ± karÅŸÄ±laÅŸtÄ±rmak
    4. En iyi ve en kÃ¶tÃ¼ modellerin tÃ¼m tekniklerle birlikte performansÄ±nÄ± analiz etmek
    
    ---
    
    ### ğŸ¯ Senaryo YapÄ±sÄ±
    
    """)
    
    # Scenario overview table
    scenario_data = {
        'Senaryo': ['S0', 'S1', 'S2', 'S3', 'S4', 'S5'],
        'Ä°sim': ['Baseline', '+ PCA', '+ FE', '+ SMOTE', '+ Optuna', 'All Combined'],
        'Scaler': ['RobustScaler', 'StandardScaler', 'RobustScaler', 'RobustScaler', 'RobustScaler', 'StandardScaler'],
        'Teknikler': ['Temel', 'PCA', 'FE', 'SMOTE', 'Optuna', 'FE+PCA+SMOTE+Optuna'],
        'Modeller': [6, 6, 6, 6, 6, 2]
    }
    
    st.dataframe(pd.DataFrame(scenario_data), use_container_width=True)
    
    st.markdown("""
    ---
    
    ### ğŸ“ˆ Ã–zet SonuÃ§lar
    
    """)
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ğŸ† En Ä°yi Senaryo", "S5: All Combined", "F1: 0.843")
    
    with col2:
        st.metric("ğŸ“Š En Etkili Teknik", "SMOTE", "+3.8% F1")
    
    with col3:
        st.metric("ğŸš€ En Ã‡ok GeliÅŸen", "XGBoost", "+10.2%")
    
    with col4:
        st.metric("ğŸ¯ Ã–nerilen Model", "Logistic Reg.", "Recall: 0.824")
    
    st.markdown("""
    ---
    
    ### ğŸ” Veri Seti Ã–zellikleri
    
    - **Kaynak:** UCI Machine Learning Repository
    - **Veri Seti:** Cleveland Heart Disease
    - **Ã–rneklem:** 304 hastaya ait 13 klinik parametre
    - **Hedef:** Binary sÄ±nÄ±flandÄ±rma (SaÄŸlÄ±klÄ± vs Hasta)
    - **SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:** 54.3% SaÄŸlÄ±klÄ±, 45.7% Hasta
    
    **Ã–zellikler:**
    - age, sex, cp, trestbps, chol, fbs, restecg, thalch, exang, oldpeak, slope, ca, thal
    
    **MÃ¼hendislik Ã–zellikleri (S2, S5):**
    - risk_score, age_group, hr_age_ratio, bp_chol_interaction
    
    ---
    
    ### ğŸ› ï¸ KullanÄ±lan Teknikler
    
    **Veri Ã–niÅŸleme:**
    - KNN Imputer: Eksik deÄŸerleri benzer Ã¶rneklerden doldur
    - RobustScaler: AykÄ±rÄ± deÄŸerlere dayanÄ±klÄ± Ã¶lÃ§ekleme
    - StandardScaler: Normal daÄŸÄ±lÄ±m iÃ§in Ã¶lÃ§ekleme
    
    **Boyut Azaltma:**
    - PCA: %95 varyans ile boyut azaltma
    
    **SÄ±nÄ±f Dengeleme:**
    - SMOTE: Yapay Ã¶rnek oluÅŸturarak dengeleme
    
    **Optimizasyon:**
    - Optuna: TPE (Bayesian) hiperparametre optimizasyonu
    
    **Validasyon:**
    - 10-Fold Stratified Cross-Validation
    
    ---
    
    ### ğŸ“– NasÄ±l KullanÄ±lÄ±r?
    
    **Sol menÃ¼den sayfa seÃ§in:**
    1. **Senaryo Analizi:** 6 senaryonun detaylÄ± analizi
    2. **KarÅŸÄ±laÅŸtÄ±rma:** TÃ¼m senaryolarÄ±n performans karÅŸÄ±laÅŸtÄ±rmasÄ±
    3. **Heatmap:** Model Ã— Senaryo F1-Score heatmap
    4. **Hasta Prediksiyon:** Yeni hasta verisi ile tahmin
    5. **Model Ã–nerileri:** FarklÄ± senaryolar iÃ§in Ã¶neriler
    6. **Teknik DokÃ¼mantasyon:** DetaylÄ± teknik bilgiler
    """)

def page_scenarios():
    """Scenario analysis page"""
    st.markdown("""
    <div class="scenario-header">
        <h1>ğŸ“Š 6 Senaryo DetaylÄ± Analizi</h1>
        <p>Her senaryonun 6 model ile performans karÅŸÄ±laÅŸtÄ±rmasÄ±</p>
    </div>
    """, unsafe_allow_html=True)
    
    df = load_cleveland_data()
    if df is None:
        st.stop()
    
    scenario = st.selectbox(
        "Senaryo SeÃ§in:",
        ["S0: Baseline", "S1: + PCA", "S2: + Feature Engineering", 
         "S3: + SMOTE", "S4: + Optuna", "S5: All Combined"],
        index=0
    )
    
    # ========== S0: Baseline ==========
    if scenario == "S0: Baseline":
        with st.expander("ğŸ“‹ Senaryo S0 DetaylarÄ±", expanded=True):
            st.markdown("""
            **KonfigÃ¼rasyon:**
            - Scaler: RobustScaler (aykÄ±rÄ± deÄŸerlere dayanÄ±klÄ±)
            - Feature Engineering: âŒ Yok
            - Boyut Azaltma: âŒ Yok
            - SÄ±nÄ±f Dengeleme: âŒ Yok
            - Hiperparametre Optim.: âŒ Yok
            - Validasyon: 10-Fold Stratified CV
            - Modeller: 6 adet (varsayÄ±lan parametrelerle)
            
            **RobustScaler FormÃ¼lÃ¼:** `(X - median) / IQR`
            
            **AvantajÄ±:** AykÄ±rÄ± deÄŸerlere karÅŸÄ± dayanÄ±klÄ± Ã¶lÃ§ekleme
            """)
        
        with st.spinner("S0 Baseline analiz Ã§alÄ±ÅŸÄ±yor..."):
            results_0, features_0, scaler_info = scenario_0_baseline(df)
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("ğŸ“Š Model SonuÃ§larÄ±")
            plot_results_table(results_0)
        
        with col2:
            st.subheader("ğŸ“ˆ F1-Score KarÅŸÄ±laÅŸtÄ±rmasÄ±")
            plot_f1_comparison(results_0, "S0: Baseline F1-Scores")
        
        plot_metrics_comparison(results_0, "S0: TÃ¼m Metriklerin KarÅŸÄ±laÅŸtÄ±rmasÄ±")
        
        # Summary
        valid = {k: v for k, v in results_0.items() if v['f1_mean'] > 0}
        if valid:
            best_model = max(valid.items(), key=lambda x: x[1]['f1_mean'])
            worst_model = min(valid.items(), key=lambda x: x[1]['f1_mean'])
            
            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"""
                <div class="success-box">
                <b>ğŸ† En Ä°yi Model:</b> {best_model[0]}<br>
                <b>F1-Score:</b> {best_model[1]['f1']}<br>
                <b>AUC:</b> {best_model[1]['auc']}
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                st.markdown(f"""
                <div class="info-box">
                <b>ğŸ“‰ En KÃ¶tÃ¼ Model:</b> {worst_model[0]}<br>
                <b>F1-Score:</b> {worst_model[1]['f1']}<br>
                <b>AUC:</b> {worst_model[1]['auc']}
                </div>
                """, unsafe_allow_html=True)
    
    # ========== S1: + PCA ==========
    elif scenario == "S1: + PCA":
        with st.expander("ğŸ“‹ Senaryo S1 DetaylarÄ±", expanded=True):
            st.markdown("""
            **KonfigÃ¼rasyon:**
            - Scaler: StandardScaler (PCA iÃ§in gerekli)
            - PCA: n_components=0.95 (%95 varyans)
            - Feature Engineering: âŒ Yok
            - SÄ±nÄ±f Dengeleme: âŒ Yok
            - Hiperparametre Optim.: âŒ Yok
            - Validasyon: 10-Fold Stratified CV
            
            **PCA (Principal Component Analysis) Nedir?**
            - Boyut azaltma tekniÄŸi
            - VaryansÄ± korurken features azaltÄ±r
            - Hesaplama hÄ±zÄ±nÄ± artÄ±rÄ±r
            - Multicollinearity problemini Ã§Ã¶zer
            
            **Beklenen Etki:** +0.3% F1 iyileÅŸme (XGBoost +7.1%)
            """)
        
        with st.spinner("S1 PCA analiz Ã§alÄ±ÅŸÄ±yor..."):
            results_1, pca_info = scenario_1_pca(df)
        
        st.info(f"âœ“ {pca_info}")
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("ğŸ“Š Model SonuÃ§larÄ±")
            plot_results_table(results_1)
        
        with col2:
            st.subheader("ğŸ“ˆ F1-Score KarÅŸÄ±laÅŸtÄ±rmasÄ±")
            plot_f1_comparison(results_1, "S1: + PCA F1-Scores")
        
        plot_metrics_comparison(results_1, "S1: TÃ¼m Metriklerin KarÅŸÄ±laÅŸtÄ±rmasÄ±")
    
    # ========== S2: + Feature Engineering ==========
    elif scenario == "S2: + Feature Engineering":
        with st.expander("ğŸ“‹ Senaryo S2 DetaylarÄ±", expanded=True):
            st.markdown("""
            **KonfigÃ¼rasyon:**
            - Scaler: RobustScaler
            - Feature Engineering: âœ… 4 yeni Ã¶zellik
            - PCA: âŒ Yok
            - SÄ±nÄ±f Dengeleme: âŒ Yok
            - Hiperparametre Optim.: âŒ Yok
            
            **MÃ¼hendislik Ã–zellikleri:**
            1. `risk_score` = (age Ã— chol) / 10000 â†’ YaÅŸ-kolesterol risk
            2. `age_group` = Binning (0-40, 40-55, 55-70, 70+) â†’ YaÅŸ kategorileri
            3. `hr_age_ratio` = thalch / (age + 1) â†’ YaÅŸa normalize kalp hÄ±zÄ±
            4. `bp_chol_interaction` = (trestbps Ã— chol) / 10000 â†’ BP-kolesterol etkileÅŸimi
            
            **Beklenen Etki:** -0.3% F1 deÄŸiÅŸim (etkisiz, Cleveland zaten iyi tasarlanmÄ±ÅŸ)
            """)
        
        with st.spinner("S2 Feature Engineering analiz Ã§alÄ±ÅŸÄ±yor..."):
            results_2, features_2 = scenario_2_feature_engineering(df)
        
        st.success(f"âœ“ Eklenen Ã¶zellikler: {len(features_2)} features")
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("ğŸ“Š Model SonuÃ§larÄ±")
            plot_results_table(results_2)
        
        with col2:
            st.subheader("ğŸ“ˆ F1-Score KarÅŸÄ±laÅŸtÄ±rmasÄ±")
            plot_f1_comparison(results_2, "S2: + FE F1-Scores")
        
        plot_metrics_comparison(results_2, "S2: TÃ¼m Metriklerin KarÅŸÄ±laÅŸtÄ±rmasÄ±")
    
    # ========== S3: + SMOTE ==========
    elif scenario == "S3: + SMOTE":
        with st.expander("ğŸ“‹ Senaryo S3 DetaylarÄ±", expanded=True):
            st.markdown("""
            **KonfigÃ¼rasyon:**
            - Scaler: RobustScaler
            - SMOTE: âœ… SÄ±nÄ±f dengeleme
            - Feature Engineering: âŒ Yok
            - PCA: âŒ Yok
            - Hiperparametre Optim.: âŒ Yok
            
            **SMOTE (Synthetic Minority Over-sampling Technique):**
            - AzÄ±nlÄ±k sÄ±nÄ±fÄ± iÃ§in yapay Ã¶rnekler oluÅŸturur
            - k-NN ile benzer Ã¶rnekleri bulur ve interpolasyon yapar
            - SÄ±nÄ±f dengesizliÄŸini Ã§Ã¶zer
            - Modelin azÄ±nlÄ±k sÄ±nÄ±fÄ±nÄ± daha iyi Ã¶ÄŸrenmesini saÄŸlar
            
            **Etki:** SaÄŸlÄ±klÄ± 165 vs Hasta 139 â†’ 165 vs 165 (dengeli)
            
            **Beklenen Etki:** +3.8% F1 iyileÅŸme (XGBoost +9.4%) - EN ETKÄ°LÄ° TEKNÄ°K!
            """)
        
        with st.spinner("S3 SMOTE analiz Ã§alÄ±ÅŸÄ±yor..."):
            results_3, balance_info = scenario_3_smote(df)
        
        st.success(f"âœ“ SÄ±nÄ±f dengelemesi: {balance_info}")
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("ğŸ“Š Model SonuÃ§larÄ±")
            plot_results_table(results_3)
        
        with col2:
            st.subheader("ğŸ“ˆ F1-Score KarÅŸÄ±laÅŸtÄ±rmasÄ±")
            plot_f1_comparison(results_3, "S3: + SMOTE F1-Scores")
        
        plot_metrics_comparison(results_3, "S3: TÃ¼m Metriklerin KarÅŸÄ±laÅŸtÄ±rmasÄ±")
    
    # ========== S4: + Optuna ==========
    elif scenario == "S4: + Optuna":
        with st.expander("ğŸ“‹ Senaryo S4 DetaylarÄ±", expanded=True):
            st.markdown("""
            **KonfigÃ¼rasyon:**
            - Scaler: RobustScaler
            - Optuna: âœ… Hiperparametre optimizasyonu
            - Trial SayÄ±sÄ±: 15 per model
            - Optimizasyon AlgoritmasÄ±: TPE (Tree-structured Parzen Estimator)
            - Maksimize Edilen Metrik: F1-Score
            
            **Optuna (Bayesian Optimization):**
            - TPE algoritmasÄ± kullanÄ±r
            - GeÃ§miÅŸ deneylerden Ã¶ÄŸrenerek smart search yapar
            - Optimal hyperparametreler bulur
            - Her model iÃ§in farklÄ± parameter alanÄ±
            
            **Beklenen Etki:** +2.5% F1 iyileÅŸme (RF +4.1%, XGBoost +8.8%)
            """)
        
        with st.spinner("S4 Optuna analiz Ã§alÄ±ÅŸÄ±yor (biraz zaman alabilir)..."):
            results_4, best_params = scenario_4_optuna(df)
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("ğŸ“Š Model SonuÃ§larÄ±")
            plot_results_table(results_4)
        
        with col2:
            st.subheader("ğŸ“ˆ F1-Score KarÅŸÄ±laÅŸtÄ±rmasÄ±")
            plot_f1_comparison(results_4, "S4: + Optuna F1-Scores")
        
        with st.expander("ğŸ”§ Optuna - En Ä°yi Hyperparametreler"):
            for model_name, params in best_params.items():
                st.write(f"**{model_name}:**")
                st.json(params)
        
        plot_metrics_comparison(results_4, "S4: TÃ¼m Metriklerin KarÅŸÄ±laÅŸtÄ±rmasÄ±")
    
    # ========== S5: All Combined ==========
    else:  # S5: All Combined
        with st.expander("ğŸ“‹ Senaryo S5 DetaylarÄ±", expanded=True):
            st.markdown("""
            **KonfigÃ¼rasyon:**
            - Scaler: StandardScaler (PCA iÃ§in)
            - Feature Engineering: âœ… 4 yeni Ã¶zellik
            - PCA: âœ… n_components=0.95
            - SMOTE: âœ… SÄ±nÄ±f dengeleme
            - Optuna: âœ… Hiperparametre optimizasyonu (15 trial per model)
            - Validasyon: 10-Fold Stratified CV
            
            **Pipeline:**
            ```
            17 Ã¶zellik (13 orijinal + 4 engineered)
                â†“
            StandardScaler
                â†“
            PCA (12 component)
                â†“
            SMOTE (330 Ã¶rnek)
                â†“
            Optuna Optimizasyon + 10-Fold CV
            ```
            
            **Test Edilen Modeller:**
            - Logistic Regression (En iyi baseline'da)
            - XGBoost (En kÃ¶tÃ¼ baseline'da)
            
            **Beklenen Etki:** +5.0% F1 iyileÅŸme (XGBoost +10.2%) - EN Ä°YÄ° PERFORMANS!
            """)
        
        with st.spinner("S5 All Combined analiz Ã§alÄ±ÅŸÄ±yor (biraz zaman alabilir)..."):
            results_5, pipeline_info = scenario_5_all_combined(df)
        
        st.success(f"âœ“ {pipeline_info}")
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("ğŸ“Š Model SonuÃ§larÄ±")
            plot_results_table(results_5)
        
        with col2:
            st.subheader("ğŸ“ˆ F1-Score KarÅŸÄ±laÅŸtÄ±rmasÄ±")
            plot_f1_comparison(results_5, "S5: All Combined F1-Scores")
        
        plot_metrics_comparison(results_5, "S5: TÃ¼m Metriklerin KarÅŸÄ±laÅŸtÄ±rmasÄ±")

def page_comparison():
    """Scenario comparison page"""
    st.markdown("""
    <div class="scenario-header-alt">
        <h1>ğŸ“Š Senaryo KarÅŸÄ±laÅŸtÄ±rma Analizi</h1>
        <p>TÃ¼m 6 Senaryonun Performans Ã–zeti ve Teknik Etki Analizi</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Summary data
    summary_data = {
        'Senaryo': ['S0: Baseline', 'S1: + PCA', 'S2: + FE', 'S3: + SMOTE', 'S4: + Optuna', 'S5: All Combined'],
        'Scaler': ['RobustScaler', 'StandardScaler', 'RobustScaler', 'RobustScaler', 'RobustScaler', 'StandardScaler'],
        'Ortalama F1': [0.788, 0.791, 0.785, 0.826, 0.813, 0.838],
        'En Ä°yi F1': [0.817, 0.820, 0.815, 0.837, 0.824, 0.843],
        'En Ä°yi Model': ['LR', 'LR', 'LR', 'LR', 'RF', 'LR'],
        'F1 vs Baseline': ['0%', '+0.3%', '-0.3%', '+3.8%', '+2.5%', '+5.0%']
    }
    
    df_summary = pd.DataFrame(summary_data)
    
    st.subheader("ğŸ“ˆ Senaryo Ã–zet Tablosu")
    st.dataframe(df_summary, use_container_width=True)
    
    # Visualizations
    col1, col2 = st.columns([1, 1])
    
    with col1:
        fig, ax = plt.subplots(figsize=(10, 6))
        colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(df_summary)))
        ax.barh(df_summary['Senaryo'], df_summary['Ortalama F1'], color=colors, edgecolor='black', linewidth=1.5)
        ax.set_xlabel('Ortalama F1-Score', fontsize=12, fontweight='bold')
        ax.set_title('Senaryo BazÄ±nda Ortalama F1', fontsize=13, fontweight='bold')
        ax.set_xlim(0.75, 0.85)
        ax.grid(axis='x', alpha=0.3)
        
        for i, v in enumerate(df_summary['Ortalama F1']):
            ax.text(v + 0.001, i, f'{v:.3f}', va='center', fontsize=10, fontweight='bold')
        
        plt.tight_layout()
        st.pyplot(fig, use_container_width=True)
    
    with col2:
        fig, ax = plt.subplots(figsize=(10, 6))
        colors = plt.cm.plasma(np.linspace(0.2, 0.8, len(df_summary)))
        ax.barh(df_summary['Senaryo'], df_summary['En Ä°yi F1'], color=colors, edgecolor='black', linewidth=1.5)
        ax.set_xlabel('En Ä°yi F1-Score', fontsize=12, fontweight='bold')
        ax.set_title('Senaryo BazÄ±nda En Ä°yi Model F1', fontsize=13, fontweight='bold')
        ax.set_xlim(0.75, 0.85)
        ax.grid(axis='x', alpha=0.3)
        
        for i, v in enumerate(df_summary['En Ä°yi F1']):
            ax.text(v + 0.001, i, f'{v:.3f}', va='center', fontsize=10, fontweight='bold')
        
        plt.tight_layout()
        st.pyplot(fig, use_container_width=True)
    
    # Technique impact analysis
    st.subheader("ğŸ“Š Teknik BazÄ±nda Etki Analizi")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown("""
        ### ğŸ”µ PCA Etkisi
        - F1 DeÄŸiÅŸim: **+0.3%**
        - Baseline: 0.788
        - S1: 0.791
        - En Ã§ok fayda: XGBoost (+7.1%)
        - SonuÃ§: Minimal etki
        """)
    
    with col2:
        st.markdown("""
        ### ğŸŸ¢ Feature Eng. Etkisi
        - F1 DeÄŸiÅŸim: **-0.3%**
        - Baseline: 0.788
        - S2: 0.785
        - En Ã§ok fayda: Logistic Reg. (-0.2%)
        - SonuÃ§: ETKISIZ
        """)
    
    with col3:
        st.markdown("""
        ### ğŸŸ¡ SMOTE Etkisi
        - F1 DeÄŸiÅŸim: **+3.8%** â­
        - Baseline: 0.788
        - S3: 0.826
        - En Ã§ok fayda: XGBoost (+9.4%)
        - SonuÃ§: EN ETKÄ°LÄ°!
        """)
    
    with col4:
        st.markdown("""
        ### ğŸ”´ Optuna Etkisi
        - F1 DeÄŸiÅŸim: **+2.5%**
        - Baseline: 0.788
        - S4: 0.813
        - En Ã§ok fayda: RF (+4.1%)
        - SonuÃ§: Etkili
        """)
    
    # Detailed findings
    st.markdown("""
    ---
    
    ### ğŸ” DetaylÄ± Bulgular
    """)
    
    findings = {
        '1. SMOTE En Etkili Teknik': 'TÃ¼m modellerde +3.8% ortalama F1 iyileÅŸme. XGBoost iÃ§in Ã¶zellikle gÃ¼Ã§lÃ¼ (+9.4%)',
        '2. Logistic Regression TutarlÄ±': 'Her senaryoda top-2 performans. En stabil model.',
        '3. XGBoost Dramatik Ä°yileÅŸme': 'Baseline\'da en zayÄ±f (F1=0.732), S5\'de gÃ¼Ã§lÃ¼ (F1=0.834). +10.2% toplam iyileÅŸme.',
        '4. Feature Engineering Etkisiz': 'Cleveland veri seti zaten iyi tasarlanmÄ±ÅŸ. Yeni Ã¶zellikler Ã§ok katkÄ± saÄŸlamadÄ±.',
        '5. Combined YaklaÅŸÄ±m En Ä°yi': 'S5 (All Combined) en yÃ¼ksek performansÄ± saÄŸladÄ± (F1=0.843, Recall=0.824)'
    }
    
    for title, content in findings.items():
        st.markdown(f"""
        <div class="success-box">
        <b>{title}</b><br>
        {content}
        </div>
        """, unsafe_allow_html=True)

def page_heatmap():
    """Heatmap analysis page"""
    st.markdown("""
    <div class="scenario-header-alt">
        <h1>ğŸ”¥ Model Ã— Senaryo Heatmap Analizi</h1>
        <p>F1-Score DeÄŸiÅŸimlerinin GÃ¶rsel Analizi</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Heatmap data
    heatmap_data = np.array([
        [0.817, 0.820, 0.815, 0.837, 0.824, 0.843],  # LR
        [0.791, 0.789, 0.786, 0.824, 0.824, 0.834],  # RF
        [0.773, 0.795, 0.781, 0.828, 0.815, 0.825],  # SVM
        [0.767, 0.779, 0.793, 0.811, 0.798, 0.815],  # NB
        [0.732, 0.820, 0.769, 0.826, 0.820, 0.834],  # XGB
        [0.766, 0.782, 0.769, 0.827, 0.802, 0.825],  # KNN
    ])
    
    models = ['Logistic Regression', 'Random Forest', 'SVM', 'Naive Bayes', 'XGBoost', 'KNN']
    scenarios = ['S0: Baseline', 'S1: PCA', 'S2: FE', 'S3: SMOTE', 'S4: Optuna', 'S5: All']
    
    fig, ax = plt.subplots(figsize=(12, 8))
    
    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn',
                xticklabels=scenarios, yticklabels=models,
                vmin=0.70, vmax=0.85, cbar_kws={'label': 'F1-Score'},
                ax=ax, linewidths=1, linecolor='white', annot_kws={'fontsize': 11, 'fontweight': 'bold'})
    
    ax.set_title('Model Ã— Senaryo F1-Score Heatmap', fontsize=14, fontweight='bold', pad=20)
    ax.set_xlabel('Senaryo', fontsize=12, fontweight='bold')
    ax.set_ylabel('Model', fontsize=12, fontweight='bold')
    
    plt.tight_layout()
    st.pyplot(fig, use_container_width=True)
    
    # Interpretation
    st.markdown("""
    ### ğŸ“Œ Heatmap Yorumu
    
    """)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        **Renkler:**
        - ğŸŸ¢ YeÅŸil (0.82+): MÃ¼kemmel
        - ğŸŸ¡ SarÄ± (0.75-0.82): Ä°yi
        - ğŸ”´ KÄ±rmÄ±zÄ± (<0.75): ZayÄ±f
        """)
    
    with col2:
        st.markdown("""
        **GÃ¶zlemler:**
        1. LR tutarlÄ± top-2
        2. XGB dramatik S3â†’S5
        3. SMOTE tÃ¼mÃ¼nÃ¼ lift ediyor
        """)
    
    with col3:
        st.markdown("""
        **En Ä°yi Kombinasyon:**
        - Model: LR
        - Senaryo: S5
        - F1: 0.843
        - Recall: 0.824
        """)
    
    # Detailed analysis
    st.subheader("ğŸ” Model-Senaryo KombinasyonlarÄ±")
    
    st.markdown("""
    ### ğŸ† En Ä°yi Kombinasyonlar:
    1. **LR + S5:** F1=0.843 (Recall=0.824) - Ã–NERÄ°LEN
    2. **LR + S3:** F1=0.837 (Recall=0.806) - HÄ±zlÄ± alternatif
    3. **XGB + S5:** F1=0.834 - En Ã§ok geliÅŸen
    4. **SVM + S3:** F1=0.828
    5. **RF + S4:** F1=0.824
    
    ### ğŸ“‰ Sorunlu Kombinasyonlar:
    1. **XGB + S0:** F1=0.732 (Recall=0.671) - EN KÃ–TÃœ
    2. **NB + S0:** F1=0.767 (Recall=0.689)
    3. **NB + S2:** F1=0.793
    4. **KNN + S0:** F1=0.766
    5. **KNN + S2:** F1=0.769
    """)

def page_patient_prediction():
    """Patient prediction page"""
    st.markdown("""
    <div class="scenario-header">
        <h1>ğŸ¥ Hasta Tahmini ModÃ¼lÃ¼</h1>
        <p>SeÃ§ilen Senaryo ve Model ile Personalized Tahmin</p>
    </div>
    """, unsafe_allow_html=True)
    
    df = load_cleveland_data()
    
    st.subheader("ğŸ“‹ Hasta Bilgileri GiriÅŸ Formu")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        age = st.number_input("YaÅŸ", min_value=20, max_value=80, value=50, help="Hasta yaÅŸÄ± (20-80)")
        sex = st.selectbox("Cinsiyet", ["Male", "Female"], help="HastanÄ±n cinsiyeti")
        cp = st.selectbox("GÃ¶ÄŸÃ¼s AÄŸrÄ±sÄ± Tipi", 
                         ["typical angina", "atypical angina", "non-anginal", "asymptomatic"],
                         help="GÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ±nÄ±n karakteri")
    
    with col2:
        trestbps = st.number_input("Dinlenme Kan BasÄ±ncÄ± (mmHg)", min_value=80, max_value=200, value=120,
                                  help="Sistemik kan basÄ±ncÄ± (mmHg)")
        chol = st.number_input("Serum Kolesterol (mg/dl)", min_value=100, max_value=600, value=200,
                              help="Total serum kolesterol")
        fbs = st.selectbox("AÃ§lÄ±k Kan Åekeri > 120 mg/dl", [False, True],
                          help="AÃ§lÄ±k kan ÅŸekeri")
    
    with col3:
        thalch = st.number_input("Maksimum Kalp HÄ±zÄ±", min_value=60, max_value=220, value=150,
                                help="Egzersiz sÄ±rasÄ±nda ulaÅŸÄ±lan max kalp hÄ±zÄ±")
        exang = st.selectbox("Egzersiz-BaÄŸlÄ± Angina", [False, True],
                            help="Egzersiz sÄ±rasÄ±nda angina oluÅŸuyor mu?")
        oldpeak = st.number_input("ST Depresyonu", min_value=-3.0, max_value=6.0, value=1.0,
                                 help="Egzersiz neden olan ST depresyonu")
    
    col4, col5 = st.columns(2)
    
    with col4:
        restecg = st.selectbox("Dinlenme EKG", ["normal", "st-t abnormality", "lv hypertrophy"],
                              help="Dinlenme sÄ±rasÄ±nda EKG sonucu")
        slope = st.selectbox("ST Segment EÄŸimi", ["upsloping", "flat", "downsloping"],
                            help="Egzersiz ST segmentinin eÄŸimi")
    
    with col5:
        ca = st.number_input("Damar SayÄ±sÄ± (0-3)", min_value=0, max_value=3, value=0,
                            help="BÃ¼yÃ¼k damarlar (damar sayÄ±sÄ±)")
        thal = st.selectbox("Talasemi", ["normal", "fixed defect", "reversable defect"],
                           help="Talasemi tipi")
    
    # Model selection
    st.subheader("âš™ï¸ Model ve Senaryo SeÃ§imi")
    
    col1, col2 = st.columns(2)
    
    with col1:
        selected_scenario = st.selectbox(
            "Senaryo SeÃ§in:",
            ["S0: Baseline", "S1: + PCA", "S2: + FE", "S3: + SMOTE", "S4: + Optuna", "S5: All Combined"],
            help="Tahmin iÃ§in kullanÄ±lacak senaryo"
        )
    
    with col2:
        if selected_scenario == "S5: All Combined":
            selected_model = st.selectbox("Model SeÃ§in", ["Logistic Regression", "XGBoost"],
                                         help="S5'te sadece LR ve XGB test edildi")
        else:
            selected_model = st.selectbox("Model SeÃ§in", 
                                        ["Logistic Regression", "Random Forest", "SVM", 
                                         "Naive Bayes", "XGBoost", "KNN"],
                                        help="Tahmin iÃ§in kullanÄ±lacak model")
    
    show_details = st.checkbox("DetaylÄ± AÃ§Ä±klamalar", value=True, help="SonuÃ§larÄ±n detaylÄ± anlatÄ±mÄ±nÄ± gÃ¶ster")
    
    # Prediction button
    if st.button("ğŸ”® Tahmini Yap", use_container_width=True, help="Tahmin yap ve sonuÃ§larÄ± gÃ¶ster"):
        
        # Prepare data
        patient_data = {
            'age': age, 'sex': sex, 'cp': cp, 'trestbps': trestbps,
            'chol': chol, 'fbs': fbs, 'restecg': restecg, 'thalch': thalch,
            'exang': exang, 'oldpeak': oldpeak, 'slope': slope, 'ca': ca, 'thal': thal
        }
        
        # Preprocess
        df_input = pd.DataFrame([patient_data])
        
        # Encode
        le_sex = LabelEncoder().fit(['Female', 'Male'])
        le_cp = LabelEncoder().fit(['typical angina', 'atypical angina', 'non-anginal', 'asymptomatic'])
        le_restecg = LabelEncoder().fit(['normal', 'st-t abnormality', 'lv hypertrophy'])
        le_slope = LabelEncoder().fit(['upsloping', 'flat', 'downsloping'])
        le_thal = LabelEncoder().fit(['normal', 'fixed defect', 'reversable defect'])
        
        df_input['sex'] = le_sex.transform([patient_data['sex']])[0]
        df_input['cp'] = le_cp.transform([patient_data['cp']])[0]
        df_input['restecg'] = le_restecg.transform([patient_data['restecg']])[0]
        df_input['slope'] = le_slope.transform([patient_data['slope']])[0]
        df_input['thal'] = le_thal.transform([patient_data['thal']])[0]
        
        # Select model
        if selected_model == "Logistic Regression":
            model = LogisticRegression(max_iter=1000, random_state=42)
        elif selected_model == "Random Forest":
            model = RandomForestClassifier(random_state=42, n_jobs=-1)
        elif selected_model == "SVM":
            model = SVC(probability=True, random_state=42)
        elif selected_model == "Naive Bayes":
            model = GaussianNB()
        elif selected_model == "XGBoost":
            model = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss', verbosity=0)
        else:  # KNN
            model = KNeighborsClassifier(n_jobs=-1)
        
        # Train on full dataset
        df_train = df.copy()
        
        categorical_cols = ['sex', 'cp', 'restecg', 'exang', 'slope', 'thal', 'fbs']
        for col in categorical_cols:
            if col in df_train.columns:
                le = LabelEncoder()
                df_train[col] = df_train[col].fillna('missing')
                df_train[col] = le.fit_transform(df_train[col].astype(str))
        
        exclude_cols = ['id', 'num', 'target', 'dataset']
        feature_cols = [col for col in df_train.columns if col not in exclude_cols]
        
        X_train = df_train[feature_cols].values
        y_train = df_train['target'].values
        
        # âœ… CRITICAL FIX: Apply KNN Imputer BEFORE scaling to handle NaN values
        imputer = KNNImputer(n_neighbors=5)
        X_train = imputer.fit_transform(X_train)
        
        # Scale if needed
        scaler = RobustScaler()
        X_train = scaler.fit_transform(X_train)
        
        # Prepare patient input with same transformations
        df_input_scaled = df_input[feature_cols].copy().values
        df_input_scaled = imputer.transform(df_input_scaled)  # Apply same imputer
        df_input_scaled = scaler.transform(df_input_scaled)    # Apply same scaler
        
        model.fit(X_train, y_train)
        
        # Predict
        probability = model.predict_proba(df_input_scaled)[0][1]
        prediction = model.predict(df_input_scaled)[0]
        
        # Display results
        st.markdown("---")
        st.subheader("ğŸ“Š TahmÄ±n SonuÃ§larÄ±")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("â¤ï¸ HastalÄ±k OlasÄ±lÄ±ÄŸÄ±", f"{probability*100:.1f}%", 
                     delta=f"{(probability-0.5)*100:+.1f}% vs Neutral")
        
        with col2:
            if probability > 0.7:
                risk_level = "ğŸ”´ YÃœKSEK RÄ°SK"
                risk_color = "red"
            elif probability > 0.5:
                risk_level = "ğŸŸ¡ ORTA RÄ°SK"
                risk_color = "orange"
            else:
                risk_level = "ğŸŸ¢ DÃœÅÃœK RÄ°SK"
                risk_color = "green"
            st.metric("ğŸ“Š Risk Seviyesi", risk_level)
        
        with col3:
            st.metric("ğŸ¤– SeÃ§ilen Model", selected_model, delta=selected_scenario)
        
        # Detailed explanation
        if show_details:
            st.markdown("---")
            st.subheader("ğŸ“ DetaylÄ± DeÄŸerlendirme")
            
            if probability > 0.5:
                st.error("""
                ### âš ï¸ YÃœKSEK RÄ°SK DEÄERLENDÄ°RMESÄ°
                
                **Tahmin:** Model hastalÄ±k olasÄ±lÄ±ÄŸÄ±nÄ± **%70'den yÃ¼ksek** olarak belirlemiÅŸtir.
                
                **Ã–nerilen AdÄ±mlar:**
                1. âš¡ Acil olarak **kardiyolog konsÃ¼ltasyonu** alÄ±nmalÄ±
                2. ğŸ¥ EKG, stres testi ve koroner anjiyografi Ã¶nerilir
                3. ğŸ’Š Kalp saÄŸlÄ±ÄŸÄ± parametreleri hemen kontrol edilmeli
                4. ğŸš‘ AcÄ± katÄ±r tÄ±bbi takip gerekli
                5. ğŸ“‹ Risk faktÃ¶rleri (BP, kolesterol, sigara) kontrol edilmeli
                """)
            
            elif probability > 0.4:
                st.warning("""
                ### âš ï¸ ORTA RÄ°SK DEÄERLENDÄ°RMESÄ°
                
                **Tahmin:** Model hastalÄ±k olasÄ±lÄ±ÄŸÄ±nÄ± **%50-%70 arasÄ±** olarak belirlemiÅŸtir.
                
                **Ã–nerilen AdÄ±mlar:**
                1. ğŸ“ YakÄ±nda **kardiyolog randevusu** alÄ±nmalÄ±
                2. ğŸ¥ KapsamlÄ± kalp saÄŸlÄ±ÄŸÄ± deÄŸerlendirmesi yapÄ±lmalÄ±
                3. ğŸƒ Hayat tarzÄ± deÄŸiÅŸiklikleri (egzersiz, diyet) dÃ¼ÅŸÃ¼nÃ¼lmeli
                4. ğŸ“Š DÃ¼zenli izlem ve testler Ã¶nerilir
                5. ğŸ’Š Gerekli ilaÃ§lar baÅŸlanabilir
                """)
            
            else:
                st.success("""
                ### âœ… DÃœÅÃœK RÄ°SK DEÄERLENDÄ°RMESÄ°
                
                **Tahmin:** Model hastalÄ±k olasÄ±lÄ±ÄŸÄ±nÄ± **%50'den dÃ¼ÅŸÃ¼k** olarak belirlemiÅŸtir.
                
                **Ã–nerilen AdÄ±mlar:**
                1. âœ“ DÃ¼zenli saÄŸlÄ±k kontrolÃ¼ yÄ±lda bir kez
                2. ğŸ’ª SaÄŸlÄ±klÄ± yaÅŸam tarzÄ±nÄ± sÃ¼rdÃ¼r
                3. ğŸ“ˆ Risk faktÃ¶rlerini izle (BP, kolesterol, kilo)
                4. ğŸƒ DÃ¼zenli egzersiz yap
                5. ğŸ“… Ä°lk belirtilerde doktor konsultasyonu
                """)
            
            # Patient summary
            st.markdown("---")
            st.markdown("**Hasta Parametreleri Ã–zeti:**")
            
            param_summary = pd.DataFrame({
                'Parametre': ['YaÅŸ', 'Cinsiyet', 'GÃ¶ÄŸÃ¼s AÄŸrÄ±sÄ±', 'Kan BasÄ±ncÄ±', 'Kolesterol',
                             'Kalp HÄ±zÄ±', 'ST Depresyonu', 'Damar SayÄ±sÄ±'],
                'DeÄŸer': [f"{age} yÄ±l", sex, cp, f"{trestbps} mmHg", f"{chol} mg/dl",
                         f"{thalch} bpm", f"{oldpeak}", f"{ca}"],
                'Normal AralÄ±k': ['25-75', 'Erkek/KadÄ±n', 'Tip-baÄŸlÄ±dÄ±r', '90-120', '<200',
                                 '60-100', '<1.0', '0-1']
            })
            
            st.dataframe(param_summary, use_container_width=True)
            
            # Medical disclaimer
            st.markdown("""
            ---
            
            ### âš–ï¸ Yasal UyarÄ± ve Sorumluluk Reddi
            
            **Ã–NEMLÄ°:** Bu tahmin, tÄ±bbi tanÄ± aracÄ± **DEÄÄ°LDÄ°R**. SonuÃ§lar yalnÄ±zca bilgilendirme
            amaÃ§lÄ±dÄ±r ve yapay zeka tarafÄ±ndan saÄŸlanmÄ±ÅŸtÄ±r.
            
            **Kritik UyarÄ±lar:**
            - âŒ Bu model hiÃ§bir durumda doktor muayenesinin yerine geÃ§mez
            - âŒ TÄ±bbi kararlar **kesinlikle** bir doktor ile birlikte verilmeli
            - âŒ Acil durumlarda 112'yi arayÄ±n
            - âœ… Her zaman **nitelikli saÄŸlÄ±k profesyoneli** ile danÄ±ÅŸÄ±nÄ±z
            - âœ… Model sadece destekleyici bir araÃ§ olarak dÃ¼ÅŸÃ¼nÃ¼lmeli
            
            **Sorumluluk:** HastanÄ±n bu model sonuÃ§larÄ±na dayanarak verdiÄŸi tÄ±bbi kararlardan
            yapay zeka sistemi, geliÅŸtirici ve yayÄ±ncÄ± sorumlu deÄŸildir.
            """)

def page_recommendations():
    """Model recommendations page"""
    st.markdown("""
    <div class="scenario-header">
        <h1>ğŸ’¡ Model SeÃ§imi ve Ã–neriler</h1>
        <p>FarklÄ± Klinik Senaryolar iÃ§in En Ä°yi Model KombinasyonlarÄ±</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.subheader("ğŸ¯ KullanÄ±m SenaryolarÄ±na GÃ¶re Ã–neriler")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        ### ğŸ¥ Tarama ProgramlarÄ± (Screening)
        
        **Ã–nerilen Kombinasyon:**
        - Model: Logistic Regression
        - Senaryo: S5 (All Combined)
        
        **Metrikleri:**
        - F1-Score: 0.843
        - Recall: **0.824** â­
        - Precision: 0.862
        - AUC: 0.916
        
        **Neden?**
        - YÃ¼ksek Recall (hastalarÄ± yakalama)
        - Tarama amacÄ±nda FN < FP
        - Hata-toleranslÄ±
        
        **Maliyeti:** YÃ¼ksek (S5)
        """)
    
    with col2:
        st.markdown("""
        ### ğŸ’» Klinik Karar Destek (Clinical)
        
        **Ã–nerilen Kombinasyon:**
        - Model: Logistic Regression
        - Senaryo: S3 (SMOTE)
        
        **Metrikleri:**
        - F1-Score: 0.837
        - Recall: 0.806
        - AUC: 0.908
        
        **Neden?**
        - Yorumlanabilir model
        - HÄ±zlÄ± tahmin
        - TÄ±bbi aÃ§Ä±klama yapÄ±labilir
        - Ä°yi dengelenmiÅŸ sÄ±nÄ±flar
        
        **Maliyeti:** DÃ¼ÅŸÃ¼k (S3)
        """)
    
    with col3:
        st.markdown("""
        ### âš¡ SÄ±nÄ±rlÄ± Kaynak (Resource-Limited)
        
        **Ã–nerilen Kombinasyon:**
        - Model: Logistic Regression
        - Senaryo: S0 (Baseline)
        
        **Metrikleri:**
        - F1-Score: 0.817
        - HÄ±z: â­â­â­â­â­
        - Bellek: Minimum
        
        **Neden?**
        - Minimal hesaplama
        - Ã‡evrimdÄ±ÅŸÄ± Ã§alÄ±ÅŸabilir
        - Mobil uygulamada kullanÄ±labilir
        - SoÄŸuk baÅŸlangÄ±Ã§ sorunlarÄ± yok
        
        **Maliyeti:** Minimal (S0)
        """)
    
    # Detailed comparison
    st.subheader("ğŸ“Š Teknik KarÅŸÄ±laÅŸtÄ±rma")
    
    comparison_data = {
        'Kriter': [
            'Performans (F1)',
            'HÄ±z (Tahmin)',
            'Bellek KullanÄ±mÄ±',
            'Yorumlanabilirlik',
            'EÄŸitim ZamanÄ±',
            'Standart Sapma'
        ],
        'S0 (Baseline)': [
            '0.817',
            'â­â­â­â­â­',
            'â­â­â­â­â­',
            'â­â­â­â­â­',
            '< 1 sn',
            'Â±0.068'
        ],
        'S3 (SMOTE)': [
            '0.837',
            'â­â­â­â­',
            'â­â­â­â­',
            'â­â­â­â­',
            '~2 sn',
            'Â±0.075'
        ],
        'S5 (All)': [
            '0.843',
            'â­â­â­',
            'â­â­â­',
            'â­â­â­',
            '~30 sn',
            'Â±0.064'
        ]
    }
    
    st.dataframe(pd.DataFrame(comparison_data), use_container_width=True)
    
    # Decision tree
    st.markdown("""
    ---
    
    ### ğŸŒ³ Model SeÃ§im Karar AÄŸacÄ±
    
    ```
    BaÅŸla
    â”‚
    â”œâ”€ Maksimum Performans Ä°steniyor mu?
    â”‚  â”œâ”€ EVET â†’ S5 + Logistic Regression (F1=0.843)
    â”‚  â””â”€ HAYIR â†’ Devam
    â”‚
    â”œâ”€ HÄ±zlÄ± Deployment Gerekli mi?
    â”‚  â”œâ”€ EVET â†’ S0 + Logistic Regression (< 1 sn)
    â”‚  â””â”€ HAYIR â†’ Devam
    â”‚
    â”œâ”€ SÄ±nÄ±rlÄ± Kaynaklar mÄ± (Mobil, IoT)?
    â”‚  â”œâ”€ EVET â†’ S0 + Logistic Regression (Minimal bellek)
    â”‚  â””â”€ HAYIR â†’ Devam
    â”‚
    â”œâ”€ Balans Ã–nemli mi (Ä°yi F1 + Makul HÄ±z)?
    â”‚  â”œâ”€ EVET â†’ S3 + Logistic Regression (F1=0.837, hÄ±zlÄ±)
    â”‚  â””â”€ HAYIR â†’ S5
    â”‚
    Son: Senaryo ve Model SeÃ§ildi
    ```
    
    ---
    
    ### ğŸ“Œ Nihai Ã–neriler
    
    **Genel Tavsiye:**
    ```
    Logistic Regression + SMOTE (S3)
    â†“
    - F1-Score: 0.837 (Yeterli performans)
    - HÄ±z: Makul (~2 sn eÄŸitim)
    - Yorumlanabilirlik: MÃ¼kemmel
    - Klinik Uyum: Ã–zelliklere dayalÄ± aÃ§Ä±klamalar
    - Ã–nerilen Threshold: 0.40-0.45
    ```
    
    **Maksimum Performans Gerekirse:**
    ```
    Logistic Regression + All Combined (S5)
    â†“
    - F1-Score: 0.843 (Maksimum)
    - Recall: 0.824 (Hasta yakalama)
    - AUC: 0.916 (MÃ¼kemmel)
    - Maliyeti: YÃ¼ksek (~30 sn eÄŸitim)
    - Ã–nerilen Threshold: 0.40
    ```
    
    **HÄ±zlÄ± Prototype Gerekirse:**
    ```
    Logistic Regression + Baseline (S0)
    â†“
    - F1-Score: 0.817 (Kabul edilebilir)
    - EÄŸitim ZamanÄ±: < 1 saniye
    - Bellek: Minimum
    - MVP iÃ§in ideal
    - Ã–lÃ§eklendir: S3 veya S5'e geÃ§
    ```
    """)

def page_technical():
    """Technical documentation page"""
    st.markdown("""
    <div class="scenario-header-alt">
        <h1>ğŸ“š Teknik DokÃ¼mantasyon</h1>
        <p>TÃ¼m Metodoloji, Teknik Detaylar ve Matematik</p>
    </div>
    """, unsafe_allow_html=True)
    
    tabs = st.tabs(["Veri Seti", "Preprocessing", "Teknikler", "Modeller", "Metrikleri", "Referanslar"])
    
    with tabs[0]:
        st.markdown("""
        ### ğŸ“Š Veri Seti DetaylarÄ±
        
        **Cleveland Heart Disease (UCI)**
        - Kaynak: UCI Machine Learning Repository
        - Ã–rneklem: 304
        - Ã–zellikler: 13 (orijinal) + 4 (engineered) = 17
        - Hedef: Binary (0=SaÄŸlÄ±klÄ±, 1=Hasta)
        - SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±: 54.3% vs 45.7% (dengeli)
        
        **Orijinal Ã–zellikler (13):**
        | # | AdÄ± | Tip | AralÄ±k |
        |---|-----|-----|--------|
        | 1 | age | SÃ¼rekli | 28-77 yÄ±l |
        | 2 | sex | Kategorik | Male/Female |
        | 3 | cp | Kategorik | 4 tip gÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ± |
        | 4 | trestbps | SÃ¼rekli | 94-200 mmHg |
        | 5 | chol | SÃ¼rekli | 126-564 mg/dl |
        | 6 | fbs | Binary | TRUE/FALSE |
        | 7 | restecg | Kategorik | 3 kategori |
        | 8 | thalch | SÃ¼rekli | 71-202 bpm |
        | 9 | exang | Binary | TRUE/FALSE |
        | 10 | oldpeak | SÃ¼rekli | -2.6 - 6.2 |
        | 11 | slope | Kategorik | 3 kategori |
        | 12 | ca | Ordinal | 0-3 |
        | 13 | thal | Kategorik | 3-4 kategori |
        
        **MÃ¼hendislik Ã–zellikleri (4):**
        | # | FormÃ¼l | GerekÃ§e |
        |---|--------|--------|
        | 14 | risk_score = (age Ã— chol) / 10000 | YaÅŸ-kolesterol risk |
        | 15 | age_group = Binning | YaÅŸ kategorileri |
        | 16 | hr_age_ratio = thalch / (age+1) | YaÅŸa normalize HR |
        | 17 | bp_chol_inter = (trestbps Ã— chol) / 10000 | BP-chol etkileÅŸimi |
        """)
    
    with tabs[1]:
        st.markdown("""
        ### ğŸ”§ Veri Ã–niÅŸleme Pipeline
        
        **AdÄ±m 1: Dataset Filtrelemesi**
        ```
        Orijinal: 920 satÄ±r (4 alt veri seti karÄ±ÅŸÄ±k)
        â†“
        Cleveland: 304 satÄ±r (tek kaynak)
        ```
        
        **AdÄ±m 2: Kategorik Encoding**
        ```python
        from sklearn.preprocessing import LabelEncoder
        
        categorical_cols = ['sex', 'cp', 'restecg', 'exang', 'slope', 'thal', 'fbs']
        for col in categorical_cols:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col].astype(str))
        ```
        
        **AdÄ±m 3: Eksik DeÄŸer Doldurma (KNN Imputer)**
        ```python
        from sklearn.impute import KNNImputer
        
        imputer = KNNImputer(n_neighbors=5)
        df[numeric_cols] = imputer.fit_transform(df[numeric_cols])
        ```
        **Neden KNN Imputer?**
        - Benzer Ã¶rneklerin deÄŸerlerini kullanÄ±r
        - Veri daÄŸÄ±lÄ±mÄ±nÄ± korur
        - Robust yÃ¶ntemi
        
        **AdÄ±m 4: Ã–lÃ§ekleme (Senaryo-baÄŸlÄ±)**
        
        **RobustScaler (S0, S2, S3, S4):**
        ```
        X_scaled = (X - median) / IQR
        
        AvantajÄ±: AykÄ±rÄ± deÄŸerlere dayanÄ±klÄ±
        DezavantajÄ±: Standart daÄŸÄ±lÄ±m varsayÄ±mÄ± yok
        ```
        
        **StandardScaler (S1, S5):**
        ```
        X_scaled = (X - mean) / std
        
        AvantajÄ±: Normal daÄŸÄ±lÄ±m iÃ§in optimal
        DezavantajÄ±: Outlier'lere duyarlÄ±
        Zorunluluk: PCA iÃ§in gerekli
        ```
        
        **AdÄ±m 5: Boyut Azaltma (PCA - S1, S5)**
        ```python
        from sklearn.decomposition import PCA
        
        pca = PCA(n_components=0.95, random_state=42)
        X_pca = pca.fit_transform(X_scaled)
        ```
        **SonuÃ§:** 13 feature â†’ 12 component (%97.14 varyans)
        
        **AdÄ±m 6: SÄ±nÄ±f Dengeleme (SMOTE - S3, S5)**
        ```python
        from imblearn.over_sampling import SMOTE
        
        smote = SMOTE(random_state=42)
        X_resampled, y_resampled = smote.fit_resample(X, y)
        ```
        **SonuÃ§:** 165 vs 139 â†’ 165 vs 165 (dengeli)
        """)
    
    with tabs[2]:
        st.markdown("""
        ### ğŸ› ï¸ KullanÄ±lan Optimizasyon Teknikleri
        
        **1. SMOTE (Synthetic Minority Over-sampling)**
        
        Algoritma:
        ```
        1. AzÄ±nlÄ±k sÄ±nÄ±fÄ±ndaki Ã¶rneklerin k-NN'sini bul (k=5)
        2. Bu komÅŸular arasÄ±nda random seÃ§
        3. AralarÄ±nda interpolation yaparak yapay Ã¶rnek oluÅŸtur
        4. Yeni Ã¶rneÄŸi eÄŸitim setine ekle
        5. SÄ±nÄ±flar dengeli olana kadar tekrarla
        ```
        
        Etki: XGBoost'ta +9.4% F1 iyileÅŸme
        
        ---
        
        **2. PCA (Principal Component Analysis)**
        
        Algoritma:
        ```
        1. Veri matrisinin kovaryans matrisini hesapla
        2. Eigen deÄŸerleri ve eigen vektÃ¶rleri bul
        3. Eigen vektÃ¶rleri bÃ¼yÃ¼klÃ¼ÄŸe gÃ¶re sÄ±rala
        4. Ä°lk k bileÅŸeni seÃ§ (n_components=0.95)
        5. Veriyi bu bileÅŸenlere project et
        ```
        
        SonuÃ§: 13 â†’ 12 boyut, %97.14 varyans korundu
        
        ---
        
        **3. Optuna (Bayesian Hyperparameter Optimization)**
        
        Algoritma:
        ```
        1. TPE (Tree-structured Parzen Estimator) kulllan
        2. Her trial'de model eÄŸit ve performans Ã¶lÃ§
        3. GeÃ§miÅŸ trial'lar ile prior daÄŸÄ±lÄ±m oluÅŸtur
        4. Bu prior'a gÃ¶re next hyperparameter'Ä± seÃ§ (maximize F1)
        5. n_trials kadar tekrarla
        ```
        
        **Hiperparametre Arama UzaylarÄ±:**
        
        Logistic Regression:
        - C: [0.01, 10.0] log scale
        - penalty: ['l1', 'l2']
        
        Random Forest:
        - n_estimators: [50, 300]
        - max_depth: [3, 20]
        - min_samples_split: [2, 20]
        
        SVM:
        - C: [0.1, 100.0] log scale
        - kernel: ['rbf', 'poly']
        - gamma: ['scale', 'auto']
        
        XGBoost:
        - n_estimators: [50, 300]
        - max_depth: [2, 10]
        - learning_rate: [0.01, 0.3]
        
        KNN:
        - n_neighbors: [3, 21] step=2
        - weights: ['uniform', 'distance']
        - metric: ['euclidean', 'manhattan']
        """)
    
    with tabs[3]:
        st.markdown("""
        ### ğŸ¤– KullanÄ±lan Modeller
        
        **1. Logistic Regression**
        ```python
        LogisticRegression(max_iter=1000, random_state=42)
        ```
        - **Tip:** Linear, Probabilistic
        - **KarmaÅŸÄ±klÄ±k:** O(n Ã— m)
        - **Avantaj:** Yorumlanabilir, hÄ±zlÄ±, stable
        - **Dezavantaj:** Non-linear iliÅŸkileri yakalayamaz
        - **En Ä°yi Senaryosu:** S5 (F1=0.843) â­
        
        ---
        
        **2. Random Forest**
        ```python
        RandomForestClassifier(n_estimators=100, random_state=42)
        ```
        - **Tip:** Ensemble (Bagging)
        - **KarmaÅŸÄ±klÄ±k:** O(n Ã— m Ã— log n Ã— trees)
        - **Avantaj:** Non-linear, feature importance, robust
        - **Dezavantaj:** YavaÅŸ, overfitting riski
        - **En Ä°yi Senaryosu:** S4 (F1=0.824)
        
        ---
        
        **3. Support Vector Machine (SVM)**
        ```python
        SVC(kernel='rbf', probability=True, random_state=42)
        ```
        - **Tip:** Kernel-based, Geometric
        - **KarmaÅŸÄ±klÄ±k:** O(nÂ² Ã— m) - O(nÂ³ Ã— m)
        - **Avantaj:** Non-linear, high-dimensional
        - **Dezavantaj:** Ã‡ok yavaÅŸ, hyperparameter sensitive
        - **En Ä°yi Senaryosu:** S3 (F1=0.828)
        
        ---
        
        **4. Naive Bayes**
        ```python
        GaussianNB()
        ```
        - **Tip:** Probabilistic, Generative
        - **KarmaÅŸÄ±klÄ±k:** O(n Ã— m)
        - **Avantaj:** Ã‡ok hÄ±zlÄ±, kÃ¼Ã§Ã¼k veri setleri
        - **Dezavantaj:** Conditional independence varsayÄ±mÄ±
        - **En Ä°yi Senaryosu:** S2 (F1=0.793)
        
        ---
        
        **5. XGBoost**
        ```python
        XGBClassifier(n_estimators=100, random_state=42)
        ```
        - **Tip:** Ensemble (Boosting)
        - **KarmaÅŸÄ±klÄ±k:** O(n Ã— m Ã— trees Ã— log n)
        - **Avantaj:** En gÃ¼Ã§lÃ¼, feature importance
        - **Dezavantaj:** YavaÅŸ, overfitting riski, tuning zor
        - **En Ä°yi Senaryosu:** S5 (F1=0.834)
        - **Not:** S0'da en zayÄ±f (F1=0.732), S5'de gÃ¼Ã§lÃ¼!
        
        ---
        
        **6. K-Nearest Neighbors (KNN)**
        ```python
        KNeighborsClassifier(n_neighbors=5, n_jobs=-1)
        ```
        - **Tip:** Instance-based, Lazy learning
        - **KarmaÅŸÄ±klÄ±k:** O(n Ã— m) - prediction
        - **Avantaj:** Basit, non-parametric
        - **Dezavantaj:** Ã–lÃ§eklemeye duyarlÄ±, yavaÅŸ
        - **En Ä°yi Senaryosu:** S3 (F1=0.827)
        """)
    
    with tabs[4]:
        st.markdown("""
        ### ğŸ“Š Performans Metrikleri
        
        **1. Accuracy (DoÄŸruluk)**
        ```
        Accuracy = (TP + TN) / (TP + TN + FP + FN)
        
        Yorumu: TÃ¼m tahminlerin yÃ¼zde kaÃ§Ä± doÄŸru
        Avantaj: Basit, sezgisel
        Dezavantaj: SÄ±nÄ±f dengesizliÄŸinde yanÄ±ltÄ±cÄ±
        TÄ±bbi: âŒ KullanmayÄ±n
        ```
        
        ---
        
        **2. Recall (DuyarlÄ±lÄ±k)**
        ```
        Recall = TP / (TP + FN)
        
        Yorumu: GerÃ§ek hasta olan kaÃ§Ä±nÄ±n modeli buldu
        Avantaj: False Negative'i minimize eder
        Dezavantaj: False Positive'i ignore eder
        TÄ±bbi: âœ… TIP II HATA KRÄ°TÄ°K - EN Ã–NEMLÄ°
        ```
        
        **TÄ±bbi BaÄŸlam:**
        - FN (HastayÄ± saÄŸlÄ±klÄ± deme): MuhasÄ±r - KABUL EDÄ°LMEZ
        - FP (SaÄŸlÄ±klÄ±yÄ± hasta deme): Gereksiz test - KABUL EDÄ°LBÄ°LÄ°R
        
        ---
        
        **3. Precision (Kesinlik)**
        ```
        Precision = TP / (TP + FP)
        
        Yorumu: Hasta diye tahmin ettiklerinin ne kadarÄ± hasta
        Avantaj: False Positive'i minimize eder
        Dezavantaj: False Negative'i ignore eder
        TÄ±bbi: âš ï¸ Dengeli Ã¶nem
        ```
        
        ---
        
        **4. F1-Score**
        ```
        F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
        
        Yorumu: Precision ve Recall'un harmonic mean'i
        Avantaj: SÄ±nÄ±f dengesizliÄŸinde gÃ¼venilir
        Dezavantaj: Yok
        TÄ±bbi: âœ… Ã–NERILEN - Recall'u aÄŸÄ±r basÄ±lÄ± tutarak optimize
        ```
        
        ---
        
        **5. AUC-ROC**
        ```
        AUC = Area Under ROC Curve
        
        Yorumu: True Positive Rate vs False Positive Rate
        AralÄ±k: 0.5 (rastgele) - 1.0 (mÃ¼kemmel)
        Avantaj: Probability threshold'tan baÄŸÄ±msÄ±z
        Dezavantaj: SÄ±nÄ±f dengesizliÄŸinde sorunlu olabilir
        TÄ±bbi: âš ï¸ TamamlayÄ±cÄ± metrik
        ```
        
        ---
        
        ### ğŸ¯ Model SeÃ§im Kriterleri
        
        **TÄ±bbi Tarama ProgramÄ±nda (Screening):**
        1. **Recall** > 0.80 (hastalarÄ±n %80'ini bulmalÄ±)
        2. **F1-Score** > 0.80 (dengeli performans)
        3. **Precision** > 0.75 (yanlÄ±ÅŸ alarm %25'de)
        
        **Klinik Destek Sisteminde:**
        1. **Yorumlanabilirlik** maksimum
        2. **F1-Score** > 0.75
        3. **HÄ±z** < 1 saniye/tahmini
        
        **TeÅŸhis DoÄŸrulamasÄ±nda (Confirmation):**
        1. **Precision** > 0.95 (yanlÄ±ÅŸ alarmÄ± minimize)
        2. **F1-Score** > 0.70
        3. Ä°nsan hekimle birlikte kullanÄ±m
        """)
    
    with tabs[5]:
        st.markdown("""
        ### ğŸ“š Referanslar ve Kaynaklar
        
        **Veri Seti:**
        - UCI Machine Learning Repository - Heart Disease Dataset
        - https://archive.ics.uci.edu/ml/datasets/heart+disease
        
        **SMOTE:**
        - Chawla, N. V., et al. (2002). SMOTE: Synthetic Minority Over-sampling Technique
        - Journal of Artificial Intelligence Research, 16, 321-357
        
        **Optuna:**
        - Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019)
        - Optuna: A Next-generation Hyperparameter Optimization Framework
        - arXiv preprint arXiv:1907.10902
        
        **Makine Ã–ÄŸrenmesi AlgoritmalarÄ±:**
        - Hastie, T., Tibshirani, R., & Friedman, J. (2009)
        - The Elements of Statistical Learning (2nd ed.)
        - Springer
        
        **Cross-Validation:**
        - Kohavi, R. (1995). A Study of Cross-Validation and Bootstrap for Accuracy Estimation
        - and Model Selection. In IJCAI, 14(2), 1137-1145
        
        **Kalp HastalÄ±ÄŸÄ± Ä°statistikleri:**
        - WHO - Cardiovascular diseases (CVDs) - Fact sheets
        - European Heart Journal - Clinical practice guidelines
        
        **PCA:**
        - Jolliffe, I. T. (2002). Principal Component Analysis (2nd ed.)
        - Springer-Verlag
        
        **Python KÃ¼tÃ¼phaneleri:**
        - scikit-learn >= 1.0.0
        - imbalanced-learn >= 0.9.0
        - optuna >= 3.0.0
        - xgboost >= 1.5.0
        - pandas >= 1.3.0
        - numpy >= 1.20.0
        - matplotlib >= 3.4.0
        - seaborn >= 0.11.0
        - streamlit >= 1.10.0
        """)

# ============================================================================
# MAIN APP ROUTER
# ============================================================================

def main():
    # Sidebar navigation
    st.sidebar.markdown("---")
    # st.sidebar.image("https://via.placeholder.com/200x100?text=Heart+Disease", use_container_width=True)
    st.sidebar.markdown("### â¤ï¸ Heart Disease Prediction System")
    st.sidebar.markdown("---")
    
    page = st.sidebar.radio(
        "ğŸ“– **SayfalarÄ± SeÃ§:**",
        [
            "ğŸ  Ana Sayfa",
            "ğŸ“Š Senaryo Analizi",
            "ğŸ“ˆ KarÅŸÄ±laÅŸtÄ±rma",
            "ğŸ”¥ Heatmap",
            "ğŸ¥ Hasta Prediksiyon",
            "ğŸ’¡ Model Ã–nerileri",
            "ğŸ“š Teknik DokÃ¼mantasyon"
        ]
    )
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("""
    **ğŸ“Š Proje Bilgisi:**
    - Veri: UCI Heart Disease
    - Senaryo: 6 (S0-S5)
    - Model: 6 + optimization
    - Metrik: Accuracy, Recall, F1, AUC
    
    **âœ… Ã–nerilen Model:**
    - Logistic Regression + S3 (SMOTE)
    - F1: 0.837 | Recall: 0.806
    
    **ğŸ¯ En Ä°yi Performans:**
    - Logistic Regression + S5 (All Combined)
    - F1: 0.843 | Recall: 0.824
    """)
    
    # Route to pages
    if page == "ğŸ  Ana Sayfa":
        page_home()
    elif page == "ğŸ“Š Senaryo Analizi":
        page_scenarios()
    elif page == "ğŸ“ˆ KarÅŸÄ±laÅŸtÄ±rma":
        page_comparison()
    elif page == "ğŸ”¥ Heatmap":
        page_heatmap()
    elif page == "ğŸ¥ Hasta Prediksiyon":
        page_patient_prediction()
    elif page == "ğŸ’¡ Model Ã–nerileri":
        page_recommendations()
    else:  # Technical Documentation
        page_technical()
    
    # Footer
    st.markdown("---")
    st.markdown("""
    <center>
    
    **â¤ï¸ UCI Heart Disease Prediction System**
    
    Yapay Zeka destekli KardiyovaskÃ¼ler Risk Tahmini
    
    _Bu sistem eÄŸitim ve araÅŸtÄ±rma amaÃ§lÄ±dÄ±r. TÄ±bbi tanÄ± aracÄ± deÄŸildir._
    
    **Daima bir doktor ile danÄ±ÅŸÄ±nÄ±z.**
    
    </center>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
